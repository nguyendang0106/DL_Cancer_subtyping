# üìö L√Ω Thuy·∫øt & C√¥ng Th·ª©c To√°n H·ªçc - Omics Cancer Classification

## M·ª•c L·ª•c

1. [Preprocessing Methods](#1-preprocessing-methods)
2. [Feature Selection](#2-feature-selection)
3. [Classical ML Models](#3-classical-ml-models)
4. [Deep Learning](#4-deep-learning)
5. [Graph Neural Networks](#5-graph-neural-networks)
6. [Model Interpretation](#6-model-interpretation)
7. [Advanced Methods](#7-advanced-methods)

---

## 1. PREPROCESSING METHODS

### 1.1 Log Transformation

**C√¥ng th·ª©c:**

```
X_transformed = log‚ÇÇ(X + 1)
```

**L√Ω do:**

- RNA-seq data c√≥ ph√¢n ph·ªëi l·ªách ph·∫£i (right-skewed)
- Log transform l√†m cho data g·∫ßn v·ªõi ph√¢n ph·ªëi normal
- ·ªîn ƒë·ªãnh variance
- +1 ƒë·ªÉ tr√°nh log(0)

**Khi n√†o s·ª≠ d·ª•ng:**

- RNA-seq count data
- Proteomics abundance data
- B·∫•t k·ª≥ data n√†o c√≥ large dynamic range

---

### 1.2 TPM Normalization (Transcripts Per Million)

**C√¥ng th·ª©c:**

```
Step 1: RPK_i = reads_i / (gene_length_i / 1000)
Step 2: TPM_i = (RPK_i / Œ£_j RPK_j) √ó 10‚Å∂
```

**L√Ω do:**

- Chu·∫©n h√≥a cho gene length
- Chu·∫©n h√≥a cho library size (sequencing depth)
- Comparable across samples

**Alternative: CPM (Counts Per Million)**

```
CPM_i = (reads_i / total_reads) √ó 10‚Å∂
```

---

### 1.3 Z-score Normalization

**C√¥ng th·ª©c:**

```
z = (x - Œº) / œÉ

Trong ƒë√≥:
- Œº = mean(X)
- œÉ = std(X)
```

**Properties:**

- Mean = 0
- Standard deviation = 1
- Preserves outliers

**L√Ω do:**

- ƒê∆∞a features v·ªÅ c√πng scale
- Quan tr·ªçng cho SVM, Neural Networks, PCA
- Gi√∫p gradient descent converge nhanh h∆°n

---

### 1.4 ComBat Batch Effect Correction

**Model:**

```
Y_ijg = Œ±_g + X Œ≤_g + Œ≥_ig + Œ¥_ig Œµ_ijg

Trong ƒë√≥:
- Y_ijg: Expression c·ªßa gene g trong sample j c·ªßa batch i
- Œ±_g: Overall gene expression
- X Œ≤_g: Covariate effects
- Œ≥_ig: Additive batch effect
- Œ¥_ig: Multiplicative batch effect
- Œµ_ijg: Error term
```

**Empirical Bayes Estimation:**

```
Œ≥ÃÇ_ig ~ N(Œ≥ÃÑ_g, œÑ¬≤_g)
Œ¥ÃÇ¬≤_ig ~ Inverse-Gamma(Œª_g, Œ∏_g)
```

**Corrected values:**

```
Y*_ijg = (Y_ijg - Œ±ÃÇ_g - X Œ≤ÃÇ_g - Œ≥ÃÇ_ig) / Œ¥ÃÇ_ig + Œ±ÃÇ_g + X Œ≤ÃÇ_g
```

**L√Ω do:**

- Technical variation gi·ªØa c√°c batch (different labs, platforms, time)
- Preserve biological variation
- Better than simple centering/scaling

---

### 1.5 Quantile Normalization

**Algorithm:**

1. Rank expression values trong m·ªói sample
2. Replace rank i values v·ªõi mean c·ªßa all rank i values
3. Rearrange v·ªÅ original order

**Effect:**

- ƒê·∫£m b·∫£o identical distribution across samples
- Removes technical variation
- Common trong microarray data

---

## 2. FEATURE SELECTION

### 2.1 Variance-Based Selection

**C√¥ng th·ª©c:**

```
Var(X_i) = E[(X_i - Œº_i)¬≤] = (1/n) Œ£‚±º (x_ij - Œº_i)¬≤
```

**Decision rule:**

- Keep features v·ªõi Var(X_i) > threshold
- Or: Keep top k features v·ªõi highest variance

**Pros:** Fast, unsupervised
**Cons:** Kh√¥ng x√©t correlation v·ªõi target

---

### 2.2 ANOVA F-test

**C√¥ng th·ª©c:**

```
F = MSB / MSW

MSB = (Between-group variance) = [Œ£·µ¢ n·µ¢(»≥·µ¢ - »≥)¬≤] / (k-1)
MSW = (Within-group variance) = [Œ£·µ¢Œ£‚±º (y·µ¢‚±º - »≥·µ¢)¬≤] / (N-k)

Trong ƒë√≥:
- k: Number of groups (cancer types)
- N: Total samples
- n·µ¢: Samples in group i
- »≥·µ¢: Mean of group i
- »≥: Overall mean
```

**p-value:**

```
p = P(F > F_observed | H‚ÇÄ: Œº‚ÇÅ = Œº‚ÇÇ = ... = Œº‚Çñ)
```

**Decision rule:**

- Reject H‚ÇÄ if p < Œ± (typically 0.05)
- Keep features v·ªõi smallest p-values

---

### 2.3 Mutual Information

**C√¥ng th·ª©c:**

```
I(X;Y) = Œ£‚ÇìŒ£·µß p(x,y) log[p(x,y) / (p(x)p(y))]
      = H(Y) - H(Y|X)
      = H(X) + H(Y) - H(X,Y)

Trong ƒë√≥:
- H(Y) = -Œ£·µß p(y) log p(y)  (Entropy)
- H(Y|X) = -Œ£‚ÇìŒ£·µß p(x,y) log p(y|x)  (Conditional entropy)
```

**Properties:**

- I(X;Y) ‚â• 0
- I(X;Y) = 0 if X, Y independent
- I(X;Y) = I(Y;X) (symmetric)

**L√Ω do:**

- Captures non-linear dependencies
- Kh√¥ng gi·∫£ ƒë·ªãnh ph√¢n ph·ªëi c·ª• th·ªÉ
- Better than correlation cho complex relationships

---

### 2.4 Lasso (L1 Regularization)

**Optimization problem:**

```
min_w [1/(2n) ||Xw - y||¬≤ + Œ±||w||‚ÇÅ]

||w||‚ÇÅ = Œ£·µ¢ |w·µ¢|  (L1 norm)
```

**Solution characteristics:**

- Sparse solution: Many w·µ¢ = 0 (automatic feature selection)
- Convex optimization
- Feature selection + regularization

**Vs L2 (Ridge):**

```
L2: min_w [1/(2n) ||Xw - y||¬≤ + Œ±||w||¬≤‚ÇÇ]
||w||‚ÇÇ¬≤ = Œ£·µ¢ w·µ¢¬≤
```

- L2 shrinks weights nh∆∞ng rarely = 0
- L1 drives weights to exactly 0

---

### 2.5 Principal Component Analysis (PCA)

**Mathematical formulation:**

**Step 1: Standardize data**

```
X_std = (X - Œº) / œÉ
```

**Step 2: Covariance matrix**

```
C = (1/n) X_std^T X_std
```

**Step 3: Eigen decomposition**

```
C = V Œõ V^T

Trong ƒë√≥:
- V: Eigenvectors (principal components)
- Œõ: Eigenvalues (variance explained)
```

**Step 4: Projection**

```
X_pca = X_std V[:, :k]

k: Number of components (explained variance ‚â• threshold)
```

**Explained variance ratio:**

```
EVR_k = Œª‚Çñ / Œ£·µ¢ Œª·µ¢
```

**Cumulative explained variance:**

```
CEV_k = Œ£·µ¢‚Çå‚ÇÅ·µè Œª·µ¢ / Œ£·µ¢ Œª·µ¢
```

---

## 3. CLASSICAL ML MODELS

### 3.1 Support Vector Machine (SVM)

**Primal problem (Linear SVM):**

```
min_{w,b} [1/2 ||w||¬≤ + C Œ£·µ¢ Œæ·µ¢]

subject to:
  y·µ¢(w^T x·µ¢ + b) ‚â• 1 - Œæ·µ¢  ‚àÄi
  Œæ·µ¢ ‚â• 0  ‚àÄi
```

**Decision function:**

```
f(x) = sign(w^T x + b)
```

**Dual problem:**

```
max_Œ± [Œ£·µ¢ Œ±·µ¢ - 1/2 Œ£·µ¢ Œ£‚±º Œ±·µ¢Œ±‚±ºy·µ¢y‚±ºx·µ¢^T x‚±º]

subject to:
  0 ‚â§ Œ±·µ¢ ‚â§ C  ‚àÄi
  Œ£·µ¢ Œ±·µ¢y·µ¢ = 0
```

**Kernel trick:**

```
K(x·µ¢, x‚±º) = œÜ(x·µ¢)^T œÜ(x‚±º)

Linear: K(x,x') = x^T x'
RBF: K(x,x') = exp(-Œ≥||x-x'||¬≤)
Polynomial: K(x,x') = (x^T x' + c)^d
```

**Hyperparameters:**

- C: Regularization (trade-off margin vs errors)
  - Large C: Hard margin (risk overfitting)
  - Small C: Soft margin (better generalization)
- Œ≥ (for RBF): Kernel width
  - Large Œ≥: Tight fit (overfit risk)
  - Small Œ≥: Smooth decision boundary

---

### 3.2 XGBoost (Extreme Gradient Boosting)

**Objective function:**

```
obj^(t) = Œ£·µ¢ L(y·µ¢, ≈∑·µ¢^(t-1) + f‚Çú(x·µ¢)) + Œ©(f‚Çú)

Œ©(f) = Œ≥T + 1/2 Œª Œ£‚±º w‚±º¬≤

Trong ƒë√≥:
- L: Loss function (e.g., log loss cho classification)
- f‚Çú: New tree at iteration t
- T: Number of leaves
- w‚±º: Leaf weights
- Œ≥: Complexity penalty
- Œª: L2 regularization
```

**Boosting update:**

```
≈∑^(t) = ≈∑^(t-1) + Œ∑ f‚Çú(x)

Œ∑: Learning rate (shrinkage)
```

**Split finding (gradient-based):**

```
Gain = [GL¬≤/(HL + Œª) + GR¬≤/(HR + Œª) - (GL + GR)¬≤/(HL + HR + Œª)] / 2 - Œ≥

Trong ƒë√≥:
- GL, GR: Sum of gradients (left, right)
- HL, HR: Sum of hessians (left, right)
```

**Key hyperparameters:**

- n_estimators: Number of trees
- max_depth: Tree depth (control overfitting)
- learning_rate (Œ∑): Step size (0.01-0.3)
- subsample: Row sampling ratio
- colsample_bytree: Feature sampling ratio
- lambda, alpha: L2, L1 regularization

---

### 3.3 Logistic Regression (L1)

**Model:**

```
p(y=1|x) = œÉ(w^T x + b) = 1 / (1 + exp(-(w^T x + b)))
```

**Loss function (Binary Cross-Entropy):**

```
L = -Œ£·µ¢ [y·µ¢ log(p·µ¢) + (1-y·µ¢)log(1-p·µ¢)]
```

**With L1 regularization:**

```
min_w [-Œ£·µ¢ y·µ¢ log(p·µ¢) + (1-y·µ¢)log(1-p·µ¢) + Œ±||w||‚ÇÅ]
```

**Multi-class (Softmax):**

```
p(y=k|x) = exp(w‚Çñ^T x) / Œ£‚±º exp(w‚±º^T x)

Loss = -Œ£·µ¢ Œ£‚Çñ y·µ¢‚Çñ log(p·µ¢‚Çñ)
```

**Sparse solution:**

- L1 penalty drives many weights to exactly 0
- Automatic feature selection
- Interpretable

---

## 4. DEEP LEARNING

### 4.1 Multi-Layer Perceptron (MLP)

**Forward propagation:**

```
Layer l:
  z^(l) = W^(l) a^(l-1) + b^(l)
  a^(l) = œÉ(z^(l))

Trong ƒë√≥:
- a^(0) = x (input)
- W^(l): Weight matrix
- b^(l): Bias vector
- œÉ: Activation function
```

**Activation functions:**

**ReLU:**

```
œÉ(z) = max(0, z)

Gradient: dœÉ/dz = {1 if z > 0, 0 otherwise}
```

**Sigmoid:**

```
œÉ(z) = 1 / (1 + e^(-z))

Gradient: dœÉ/dz = œÉ(z)(1 - œÉ(z))
```

**Tanh:**

```
œÉ(z) = (e^z - e^(-z)) / (e^z + e^(-z))

Gradient: dœÉ/dz = 1 - œÉ¬≤(z)
```

---

### 4.2 Batch Normalization

**C√¥ng th·ª©c:**

```
Step 1: Compute batch statistics
  Œº_B = 1/m Œ£·µ¢ x·µ¢
  œÉ¬≤_B = 1/m Œ£·µ¢ (x·µ¢ - Œº_B)¬≤

Step 2: Normalize
  xÃÇ·µ¢ = (x·µ¢ - Œº_B) / ‚àö(œÉ¬≤_B + Œµ)

Step 3: Scale and shift (learnable parameters)
  y·µ¢ = Œ≥ xÃÇ·µ¢ + Œ≤
```

**L√Ω do:**

- Reduce internal covariate shift
- Allows higher learning rates
- Regularization effect
- Stabilizes training

**Inference:**

```
Use moving average statistics:
  Œº_running = momentum √ó Œº_running + (1-momentum) √ó Œº_batch
  œÉ¬≤_running = momentum √ó œÉ¬≤_running + (1-momentum) √ó œÉ¬≤_batch
```

---

### 4.3 Dropout

**Training:**

```
For each neuron i:
  r·µ¢ ~ Bernoulli(p)
  y·µ¢ = r·µ¢ √ó x·µ¢

p: Dropout probability (typically 0.3-0.5)
```

**Inference:**

```
y = (1-p) √ó x  (Scale by keep probability)
```

**Effect:**

- Prevents co-adaptation c·ªßa neurons
- Ensemble effect (train 2^n networks implicitly)
- Regularization

---

### 4.4 Cross-Entropy Loss

**Binary:**

```
L = -Œ£·µ¢ [y·µ¢ log(≈∑·µ¢) + (1-y·µ¢)log(1-≈∑·µ¢)]
```

**Multi-class:**

```
L = -Œ£·µ¢ Œ£‚Çñ y·µ¢‚Çñ log(≈∑·µ¢‚Çñ)

Trong ƒë√≥:
- y·µ¢‚Çñ: True label (one-hot encoded)
- ≈∑·µ¢‚Çñ: Predicted probability
```

**Properties:**

- Convex (for linear models)
- Gradient well-behaved
- Probabilistic interpretation

---

### 4.5 Adam Optimizer

**Update rule:**

```
Step 1: Compute gradient
  g‚Çú = ‚àáL(Œ∏‚Çú)

Step 2: Update biased first moment
  m‚Çú = Œ≤‚ÇÅ m‚Çú‚Çã‚ÇÅ + (1-Œ≤‚ÇÅ) g‚Çú

Step 3: Update biased second moment
  v‚Çú = Œ≤‚ÇÇ v‚Çú‚Çã‚ÇÅ + (1-Œ≤‚ÇÇ) g‚Çú¬≤

Step 4: Bias correction
  mÃÇ‚Çú = m‚Çú / (1 - Œ≤‚ÇÅ·µó)
  vÃÇ‚Çú = v‚Çú / (1 - Œ≤‚ÇÇ·µó)

Step 5: Update parameters
  Œ∏‚Çú = Œ∏‚Çú‚Çã‚ÇÅ - Œ± √ó mÃÇ‚Çú / (‚àövÃÇ‚Çú + Œµ)
```

**Hyperparameters:**

- Œ±: Learning rate (default: 0.001)
- Œ≤‚ÇÅ: First moment decay (default: 0.9)
- Œ≤‚ÇÇ: Second moment decay (default: 0.999)
- Œµ: Numerical stability (default: 10‚Åª‚Å∏)

---

### 4.6 Autoencoder

**Architecture:**

```
Encoder: x ‚Üí h = f(Wx + b)
Decoder: h ‚Üí xÃÇ = g(W'h + b')

Loss: ||x - xÃÇ||¬≤
```

**Variants:**

**Denoising Autoencoder:**

```
xÃÉ = x + noise
Minimize: ||x - decoder(encoder(xÃÉ))||¬≤
```

**Variational Autoencoder (VAE):**

```
Encoder: x ‚Üí (Œº, œÉ¬≤)
Sample: z ~ N(Œº, œÉ¬≤)
Decoder: z ‚Üí xÃÇ

Loss = Reconstruction + KL Divergence
     = ||x - xÃÇ||¬≤ + KL(q(z|x) || p(z))
```

---

## 5. GRAPH NEURAL NETWORKS

### 5.1 Graph Convolutional Network (GCN)

**Layer formula:**

```
H^(l+1) = œÉ(DÃÉ^(-1/2) √É DÃÉ^(-1/2) H^(l) W^(l))

Trong ƒë√≥:
- A: Adjacency matrix
- √É = A + I (add self-loops)
- DÃÉ·µ¢·µ¢ = Œ£‚±º √É·µ¢‚±º (degree matrix)
- H^(l): Node features at layer l
- W^(l): Learnable weights
- œÉ: Activation function
```

**Intuition:**

- Average neighbor features (weighted by degree)
- Apply linear transformation
- Non-linear activation

**Message passing interpretation:**

```
h·µ¢^(l+1) = œÉ(Œ£‚±º‚ààN(i) 1/‚àö(d·µ¢d‚±º) W^(l) h‚±º^(l))
```

---

### 5.2 Graph Attention Network (GAT)

**Attention mechanism:**

```
Step 1: Compute attention coefficients
  e·µ¢‚±º = LeakyReLU(a^T [Wh·µ¢ || Wh‚±º])

Step 2: Normalize (softmax)
  Œ±·µ¢‚±º = exp(e·µ¢‚±º) / Œ£‚Çñ‚ààN(i) exp(e·µ¢‚Çñ)

Step 3: Aggregate
  h·µ¢' = œÉ(Œ£‚±º‚ààN(i) Œ±·µ¢‚±º Wh‚±º)
```

**Multi-head attention:**

```
h·µ¢' = ||‚Çñ‚Çå‚ÇÅ·¥∑ œÉ(Œ£‚±º‚ààN(i) Œ±·µ¢‚±º·µè W·µèh‚±º)

||: Concatenation
K: Number of attention heads
```

---

## 6. MODEL INTERPRETATION

### 6.1 SHAP (Shapley Values)

**Shapley value (from cooperative game theory):**

```
œÜ·µ¢(f) = Œ£_{S‚äÜF\{i}} [|S|!(|F|-|S|-1)!] / |F|! √ó [f(S‚à™{i}) - f(S)]

Trong ƒë√≥:
- F: Set of all features
- S: Subset of features
- f(S): Model prediction with feature subset S
- œÜ·µ¢: Contribution of feature i
```

**Properties:**

1. **Efficiency (accuracy):**

```
f(x) = œÜ‚ÇÄ + Œ£·µ¢ œÜ·µ¢(x)
```

2. **Symmetry:**

```
If f(S‚à™{i}) = f(S‚à™{j}) for all S, then œÜ·µ¢ = œÜ‚±º
```

3. **Dummy:**

```
If f(S‚à™{i}) = f(S) for all S, then œÜ·µ¢ = 0
```

4. **Additivity:**

```
œÜ·µ¢(f+g) = œÜ·µ¢(f) + œÜ·µ¢(g)
```

---

### 6.2 Integrated Gradients

**Formula:**

```
IG·µ¢(x) = (x·µ¢ - x'·µ¢) √ó ‚à´‚ÇÄ¬π ‚àÇf(x' + Œ±(x-x')) / ‚àÇx·µ¢ dŒ±

Riemann approximation:
IG·µ¢(x) ‚âà (x·µ¢ - x'·µ¢) √ó Œ£‚Çñ‚Çå‚ÇÅ·µê [‚àÇf(x' + k/m(x-x')) / ‚àÇx·µ¢] / m
```

**Axioms:**

1. **Sensitivity:**

```
If x·µ¢ ‚â† x'·µ¢ and f(x) ‚â† f(x'), then IG·µ¢(x) ‚â† 0
```

2. **Implementation Invariance:**

```
Functionally equivalent networks have same attributions
```

3. **Completeness:**

```
f(x) - f(x') = Œ£·µ¢ IG·µ¢(x)
```

---

### 6.3 Permutation Importance

**Algorithm:**

```
1. Compute baseline score: s_base = score(model, X, y)

2. For each feature i:
   a. Shuffle feature i: XÃÉ·µ¢ = shuffle(X·µ¢)
   b. Compute score: s·µ¢ = score(model, XÃÉ, y)
   c. Importance: Imp·µ¢ = s_base - s·µ¢

3. Repeat n_repeats times and average
```

**Statistical significance:**

```
Mean importance: Œº = mean(Imp)
Std: œÉ = std(Imp)

Significant if: Œº > k √ó œÉ (typically k=2)
```

---

## 7. ADVANCED METHODS

### 7.1 Transformer Self-Attention

**Scaled Dot-Product Attention:**

```
Attention(Q, K, V) = softmax(QK^T / ‚àöd‚Çñ) V

Trong ƒë√≥:
- Q: Query matrix
- K: Key matrix
- V: Value matrix
- d‚Çñ: Dimension of keys (for scaling)
```

**Multi-Head Attention:**

```
MultiHead(Q,K,V) = Concat(head‚ÇÅ,...,head‚Çï) W^O

head·µ¢ = Attention(QW·µ¢^Q, KW·µ¢^K, VW·µ¢^V)
```

**Positional Encoding:**

```
PE(pos, 2i) = sin(pos / 10000^(2i/d‚Çò‚Çíd‚Çë‚Çó))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d‚Çò‚Çíd‚Çë‚Çó))
```

---

### 7.2 Cox Proportional Hazards

**Model:**

```
h(t|x) = h‚ÇÄ(t) exp(Œ≤^T x)

Trong ƒë√≥:
- h(t|x): Hazard function
- h‚ÇÄ(t): Baseline hazard
- Œ≤^T x: Linear predictor (risk score)
```

**Partial Likelihood:**

```
L(Œ≤) = Œ†·µ¢ [exp(Œ≤^T x·µ¢) / Œ£‚±º‚ààR(t·µ¢) exp(Œ≤^T x‚±º)]^Œ¥·µ¢

Œ¥·µ¢: Event indicator (1=event, 0=censored)
R(t·µ¢): Risk set at time t·µ¢
```

**C-index (Concordance Index):**

```
C = #{pairs (i,j): t·µ¢ < t‚±º and risk_i > risk_j} / #{comparable pairs}

Range: [0, 1]
C=0.5: Random
C=1.0: Perfect concordance
```

---

## üìä Summary Table: When to Use What

| Method          | Best For                             | Pros                                 | Cons                        |
| --------------- | ------------------------------------ | ------------------------------------ | --------------------------- |
| **SVM**         | High-dimensional, clear margins      | Strong theory, kernel trick          | Slow for large data         |
| **XGBoost**     | Tabular data, competitions           | SOTA performance, feature importance | Black box, overfitting risk |
| **Logistic L1** | Interpretability, sparse solutions   | Simple, fast, interpretable          | Linear only                 |
| **MLP**         | Large datasets, complex patterns     | Flexible, powerful                   | Needs –º–Ω–æ–≥–æ data, black box |
| **Autoencoder** | Unsupervised pretraining             | Leverage unlabeled data              | Two-stage training          |
| **GNN**         | Graph-structured data, PPI           | Use biological knowledge             | Needs graph structure       |
| **Transformer** | Very large datasets, long-range deps | SOTA in many domains                 | Computationally expensive   |

---

## üéØ Key Takeaways

1. **Preprocessing is crucial** cho omics data (normalization, batch correction)
2. **Feature selection essential** khi n_features >> n_samples
3. **Classical ML** often competitive v·ªõi small-medium datasets
4. **Deep Learning** excels v·ªõi large datasets v√† complex patterns
5. **GNNs** leverage biological prior knowledge (pathways, PPI)
6. **Interpretation** kh√¥ng optional - always explain predictions
7. **Cross-validation** v√† external validation essential

---

**End of Theory Document** üìö
