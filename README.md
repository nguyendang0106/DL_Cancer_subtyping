# ğŸ§¬ Omics Cancer Classification Pipeline

**Pipeline hoÃ n chá»‰nh cho phÃ¢n loáº¡i ung thÆ° trÃªn dá»¯ liá»‡u omics sá»­ dá»¥ng Deep Learning vÃ  Classical Machine Learning**

---

## ğŸ“‹ Tá»•ng Quan

Pipeline nÃ y Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ xá»­ lÃ½ vÃ  phÃ¢n loáº¡i cÃ¡c loáº¡i ung thÆ° khÃ¡c nhau dá»±a trÃªn dá»¯ liá»‡u omics (genomics, transcriptomics, proteomics, methylation). Há»‡ thá»‘ng bao gá»“m:

- âœ… **Tiá»n xá»­ lÃ½ dá»¯ liá»‡u** chuyÃªn biá»‡t cho omics data
- âœ… **Feature selection** vá»›i nhiá»u phÆ°Æ¡ng phÃ¡p (ANOVA, Lasso, Mutual Information)
- âœ… **Classical ML models** (SVM, XGBoost, Logistic Regression) lÃ m baseline
- âœ… **Deep Learning models** (MLP, Autoencoder + Classifier)
- âœ… **Graph Neural Networks** sá»­ dá»¥ng PPI networks
- âœ… **Model interpretation** (SHAP, Integrated Gradients)
- âœ… **Pathway enrichment analysis**

---

## ğŸš€ Quick Start

### 1. CÃ i Ä‘áº·t dependencies

```bash
pip install -r requirements.txt
```

### 2. Cháº¡y pipeline vá»›i dá»¯ liá»‡u máº«u

```bash
cd data
python main_pipeline.py
```

### 3. Sá»­ dá»¥ng vá»›i dá»¯ liá»‡u thá»±c

```python
from main_pipeline import OmicsCancerClassificationPipeline
import pandas as pd

# Load your data
df = pd.read_csv('your_omics_data.csv', index_col=0)
X = df.drop('label', axis=1).values  # Expression matrix
y = df['label'].values  # Cancer subtypes
feature_names = df.drop('label', axis=1).columns.tolist()  # Gene names

# Run pipeline
pipeline = OmicsCancerClassificationPipeline()
comparison_df, results = pipeline.run_full_pipeline(
    X=X, y=y, feature_names=feature_names
)
```

---

## ğŸ“Š Dataset Specifications

### Äá»‹nh dáº¡ng dá»¯ liá»‡u Ä‘áº§u vÃ o

**Matrix format: `samples Ã— features`**

| Sample_ID | GENE_1 | GENE_2 | ... | GENE_N | Label |
| --------- | ------ | ------ | --- | ------ | ----- |
| Sample_1  | 125.3  | 45.2   | ... | 78.9   | 0     |
| Sample_2  | 98.7   | 52.1   | ... | 103.4  | 1     |
| ...       | ...    | ...    | ... | ...    | ...   |

### Cáº¥u hÃ¬nh máº·c Ä‘á»‹nh (config.py)

```python
DATASET_CONFIG = {
    'omics_type': 'RNA-seq',      # Loáº¡i dá»¯ liá»‡u
    'n_samples': 800,              # Sá»‘ máº«u
    'n_features': 20000,           # Sá»‘ gene/protein
    'n_classes': 5,                # Sá»‘ loáº¡i ung thÆ°
    'test_size': 0.2,              # Tá»· lá»‡ test set
    'random_state': 42
}
```

---

## ğŸ”¬ Pipeline Architecture

### 1ï¸âƒ£ **Data Preprocessing** (`data_preprocessing.py`)

#### CÃ¡c bÆ°á»›c xá»­ lÃ½:

```
Raw Data â†’ Missing Values â†’ Log Transform â†’ Batch Correction â†’
Variance Filtering â†’ Z-score Normalization â†’ Preprocessed Data
```

#### Methods:

- **Log2 Transformation**: `log2(X + 1)` - Giáº£m skewness
- **TPM/CPM Normalization**: Chuáº©n hÃ³a library size
- **Z-score**: Standardization (mean=0, std=1)
- **ComBat**: Batch effect correction
- **Variance Filtering**: Loáº¡i bá» low-variance features

#### CÃ´ng thá»©c quan trá»ng:

**Z-score Normalization:**

```
z = (x - Î¼) / Ïƒ
```

**ComBat (Empirical Bayes):**

```
X_corrected = (X - Î³_batch) / Î´_batch
```

---

### 2ï¸âƒ£ **Feature Selection** (`feature_selection.py`)

#### Methods Ä‘Æ°á»£c implement:

1. **Variance-based**: `Var(X_i) = E[(X_i - Î¼_i)Â²]`
2. **ANOVA F-test**: `F = (Between-group variance) / (Within-group variance)`
3. **Mutual Information**: `I(X;Y) = Î£ Î£ p(x,y) log[p(x,y)/(p(x)p(y))]`
4. **Lasso (L1)**: `min_w [1/2n ||Xw - y||Â² + Î±||w||â‚]`
5. **Random Forest Importance**: Gini importance
6. **PCA**: Dimensionality reduction

#### Ensemble Selection:

- **Union**: Láº¥y táº¥t cáº£ features tá»« báº¥t ká»³ method nÃ o
- **Intersection**: Chá»‰ láº¥y features Ä‘Æ°á»£c chá»n bá»Ÿi Táº¤T Cáº¢ methods
- **Majority**: Features Ä‘Æ°á»£c chá»n bá»Ÿi >50% methods

---

### 3ï¸âƒ£ **Classical ML Models** (`classical_models.py`)

#### SVM (Support Vector Machine)

**Decision function:**

```
f(x) = sign(w^T x + b)
```

**Optimization:**

```
min_w,b [1/2 ||w||Â² + C Î£ Î¾_i]
subject to: y_i(w^T x_i + b) â‰¥ 1 - Î¾_i
```

- **Kernels**: Linear, RBF
- **Best for**: High-dimensional data, clear margins

#### XGBoost

**Objective:**

```
obj(Î¸) = Î£ L(y_i, Å·_i) + Î£ Î©(f_k)
Î©(f) = Î³T + 1/2 Î»||w||Â²
```

- **Boosting**: Ensemble of trees
- **Best for**: Complex non-linear relationships

#### Logistic Regression (L1)

**Loss with L1 regularization:**

```
min_w [-Î£ y_i log(p_i) + (1-y_i)log(1-p_i) + Î±||w||â‚]
```

- **Sparse solution**: Many weights â†’ 0
- **Best for**: Interpretability, feature selection

---

### 4ï¸âƒ£ **Deep Learning Models** (`deep_learning_models.py`)

#### MLP (Multi-Layer Perceptron)

**Architecture:**

```
Input â†’ [Linear â†’ BatchNorm â†’ ReLU â†’ Dropout] Ã— N â†’ Output
```

**Components:**

1. **Batch Normalization:**

   ```
   xÌ‚ = (x - Î¼_batch) / âˆš(ÏƒÂ²_batch + Îµ)
   y = Î³xÌ‚ + Î²
   ```

2. **Dropout:**

   - Training: Randomly zero activations (p=0.3)
   - Inference: Scale by (1-p)

3. **Cross-Entropy Loss:**
   ```
   L = -Î£ Î£ y_ic log(Å·_ic)
   ```

#### Autoencoder + Classifier

**Two-stage training:**

1. **Pretrain Autoencoder (unsupervised):**

   ```
   Encoder: X â†’ Z (latent)
   Decoder: Z â†’ XÌ‚
   Loss = ||X - XÌ‚||Â²
   ```

2. **Train Classifier:**
   ```
   Encoder (frozen/fine-tuned) â†’ Classifier â†’ Predictions
   ```

**Advantages:**

- Leverage unlabeled data
- Learn compressed representations
- Better initialization

---

### 5ï¸âƒ£ **Graph Neural Networks** (`gnn_models.py`)

#### GCN (Graph Convolutional Network)

**Layer formula:**

```
H^(l+1) = Ïƒ(DÌƒ^(-1/2) Ãƒ DÌƒ^(-1/2) H^(l) W^(l))
```

**Components:**

- Ãƒ = A + I (adjacency + self-loops)
- DÌƒ: Degree matrix
- H: Node features (gene expression)

#### GAT (Graph Attention Network)

**Attention mechanism:**

```
Î±_ij = softmax_j(LeakyReLU(a^T [W h_i || W h_j]))
h_i' = Ïƒ(Î£_j Î±_ij W h_j)
```

#### PPI Network Construction:

- **Source**: STRING database
- **Nodes**: Genes/Proteins
- **Edges**: Protein-protein interactions
- **Weights**: Confidence scores (0-1)

**Why GNN for omics?**

- Capture gene-gene interactions
- Utilize biological prior knowledge
- Identify functional modules

---

### 6ï¸âƒ£ **Model Interpretation** (`model_interpretation.py`)

#### SHAP (SHapley Additive exPlanations)

**Shapley value (from game theory):**

```
Ï†_i = Î£_{SâŠ†F\{i}} [|S|!(|F|-|S|-1)!]/|F|! Ã— [f(Sâˆª{i}) - f(S)]
```

**Properties:**

- Local accuracy: `f(x) = Ï†_0 + Î£ Ï†_i`
- Consistency
- Missingness

**Interpretation:**

- Ï†_i > 0: Feature increases prediction
- Ï†_i < 0: Feature decreases prediction

#### Integrated Gradients

**Formula:**

```
IG_i(x) = (x_i - x'_i) Ã— âˆ«â‚€Â¹ âˆ‚f(x' + Î±(x-x'))/âˆ‚x_i dÎ±
```

**Riemann approximation:**

```
IG_i(x) â‰ˆ (x_i - x'_i) Ã— Î£_{k=1}^m [âˆ‚f(x^k)/âˆ‚x_i] / m
```

#### Permutation Importance

**Algorithm:**

1. Baseline score on validation set
2. For each feature: shuffle â†’ compute score â†’ importance = drop in score
3. Repeat and average

**Advantages:**

- Model-agnostic
- Accounts for interactions
- Real-world interpretation

---

## ğŸ“ˆ Evaluation Metrics

### Classification Metrics

1. **Accuracy**: `(TP + TN) / (TP + TN + FP + FN)`

2. **Precision**: `TP / (TP + FP)`

3. **Recall**: `TP / (TP + FN)`

4. **F1-score**: `2 Ã— (Precision Ã— Recall) / (Precision + Recall)`

5. **Macro F1**: Average F1 across classes (equal weight)

6. **Weighted F1**: Weighted by class support

### Cross-Validation

**Stratified K-Fold (K=5):**

- Maintains class distribution in each fold
- Reduces overfitting
- Better generalization estimate

---

## ğŸ¯ Usage Examples

### Example 1: Basic Pipeline

```python
from main_pipeline import OmicsCancerClassificationPipeline

# Initialize pipeline
pipeline = OmicsCancerClassificationPipeline()

# Run with your data
comparison_df, results = pipeline.run_full_pipeline(
    data_path='data/cancer_expression.csv'
)
```

### Example 2: Custom Configuration

```python
from config import *

# Modify configuration
PREPROCESSING_CONFIG['normalization'] = 'quantile'
FEATURE_SELECTION_CONFIG['methods'] = ['anova', 'lasso']
DL_CONFIG['mlp']['hidden_layers'] = [1024, 512, 256, 128]

# Create pipeline with custom config
pipeline = OmicsCancerClassificationPipeline()
pipeline.run_full_pipeline(X=X, y=y, feature_names=genes)
```

### Example 3: Individual Components

```python
# Just preprocessing
from data_preprocessing import OmicsPreprocessor

preprocessor = OmicsPreprocessor(PREPROCESSING_CONFIG)
X_processed = preprocessor.preprocess(X_raw, y, is_train=True)

# Just feature selection
from feature_selection import FeatureSelector

selector = FeatureSelector(FEATURE_SELECTION_CONFIG)
X_selected, indices = selector.select_features(X_processed, y)

# Just model training
from classical_models import ClassicalMLModels

clf = ClassicalMLModels(CLASSICAL_MODELS_CONFIG)
clf.train_xgboost(X_train, y_train)
results = clf.evaluate_model('xgboost', X_test, y_test)
```

---

## ğŸ”§ Configuration Guide

### Preprocessing Options

```python
PREPROCESSING_CONFIG = {
    'normalization': 'log2_tpm',    # 'log2_tpm', 'zscore', 'quantile'
    'batch_correction': 'combat',    # 'combat', 'limma', None
    'remove_low_variance': True,
    'variance_threshold': 0.01,
    'handle_missing': 'knn'          # 'mean', 'median', 'knn'
}
```

### Feature Selection Options

```python
FEATURE_SELECTION_CONFIG = {
    'methods': ['variance', 'anova', 'lasso'],
    'n_features_variance': 5000,
    'n_features_anova': 2000,
    'n_features_lasso': 1000,
    'use_pca': False,
    'pca_components': 0.95
}
```

### Deep Learning Hyperparameters

```python
DL_CONFIG = {
    'mlp': {
        'hidden_layers': [512, 256, 128, 64],
        'dropout_rate': 0.3,
        'batch_norm': True,
        'learning_rate': 0.001,
        'batch_size': 32,
        'epochs': 100,
        'early_stopping_patience': 15
    }
}
```

---

## ğŸ“Š Expected Output

### 1. Model Comparison Table

```
Model                      Type           Test Accuracy  Test F1 (Macro)
XGBOOST                    Classical ML   0.8625         0.8534
SVM                        Classical ML   0.8375         0.8289
Logistic Regression (L1)   Classical ML   0.8125         0.8012
MLP                        Deep Learning  0.8750         0.8645
Autoencoder + Classifier   Deep Learning  0.8500         0.8423
```

### 2. Top Important Features

```
Rank  Gene       Importance
1     GENE_45    0.1234
2     GENE_123   0.1156
3     GENE_789   0.0987
...
```

### 3. Visualization Files

- `model_comparison.png` - Performance comparison
- `confusion_matrix_*.png` - Per-model confusion matrices
- `shap_summary.png` - SHAP feature importance
- `training_history.png` - DL training curves

---

## ğŸ§ª Advanced Features

### 1. Multi-Omics Fusion

```python
from advanced_models import MultiOmicsFusion

# Late fusion (concatenate features)
fusion_model = MultiOmicsFusion(
    omics_types=['rna', 'protein', 'methylation'],
    fusion_type='late'
)
```

### 2. Pathway-Aware GNN

```python
from gnn_models import PPINetworkBuilder, GCNClassifier

# Build PPI network
ppi_builder = PPINetworkBuilder(confidence_threshold=0.7)
graph = ppi_builder.load_string_ppi(gene_list, species=9606)

# Train GNN
model = GCNClassifier(input_dim=1, hidden_channels=256, n_classes=5)
```

### 3. Transformer-Based Models

```python
from transformer_models import OmicsTransformer

model = OmicsTransformer(
    n_features=1000,
    n_classes=5,
    n_heads=8,
    n_layers=6,
    d_model=256
)
```

---

## ğŸ”¬ Biological Interpretation

### Pathway Enrichment Analysis

```python
from model_interpretation import ModelInterpreter

interpreter = ModelInterpreter(model, feature_names)

# Get top genes
top_genes_df = interpreter.get_top_genes(n_top=50)
top_genes = top_genes_df['Gene'].tolist()

# Enrichment analysis
enrichment = interpreter.pathway_enrichment_analysis(
    top_genes,
    organism='human'
)
```

**Output:**

- GO Biological Processes
- KEGG Pathways
- Reactome Pathways

---

## âš ï¸ Important Notes

### 1. Data Quality

- **Missing values**: Handle appropriately (KNN imputation recommended)
- **Batch effects**: Use ComBat if data from multiple batches
- **Outliers**: Check and remove if necessary

### 2. Feature Selection

- High-dimensional data (n_features >> n_samples) â†’ aggressive selection needed
- Balance between reduction and information retention
- Consider biological knowledge (pathway-based selection)

### 3. Model Selection

- **Small datasets (<500 samples)**: Classical ML often better
- **Large datasets (>1000 samples)**: DL can excel
- **Interpretability needed**: Logistic Regression, Trees
- **Performance priority**: XGBoost, Neural Networks

### 4. Evaluation

- **Always use stratified splits**: Maintain class balance
- **Cross-validation**: More robust than single split
- **Class imbalance**: Use balanced metrics (Macro F1, not just Accuracy)

---

## ğŸ“š References

### Methods

1. **Batch Correction**:

   - Johnson et al., 2007: "Adjusting batch effects in microarray expression data"

2. **SHAP**:

   - Lundberg & Lee, 2017: "A Unified Approach to Interpreting Model Predictions"

3. **Integrated Gradients**:

   - Sundararajan et al., 2017: "Axiomatic Attribution for Deep Networks"

4. **GCN**:

   - Kipf & Welling, 2017: "Semi-Supervised Classification with Graph Convolutional Networks"

5. **GAT**:
   - VeliÄkoviÄ‡ et al., 2018: "Graph Attention Networks"

### Databases

- **STRING**: Protein-protein interaction database
- **KEGG**: Pathway database
- **Reactome**: Pathway knowledge base
- **GO**: Gene Ontology

---

## ğŸ› ï¸ Troubleshooting

### Common Issues

**1. Out of Memory (OOM)**

```python
# Reduce batch size
DL_CONFIG['mlp']['batch_size'] = 16

# Use less features
FEATURE_SELECTION_CONFIG['n_features_anova'] = 500
```

**2. Overfitting**

```python
# Increase dropout
DL_CONFIG['mlp']['dropout_rate'] = 0.5

# Stronger regularization
CLASSICAL_MODELS_CONFIG['svm']['C'] = 0.1
```

**3. Poor Performance**

- Check data quality
- Try different normalization methods
- Increase feature selection threshold
- Tune hyperparameters

---

## ğŸ“ Support & Contact

For questions or issues:

- Open an issue on GitHub
- Contact: [your-email@example.com]

---

## ğŸ“„ License

MIT License - Free to use for research and education

---

## ğŸ“ Citation

If you use this pipeline in your research, please cite:

```bibtex
@software{omics_cancer_pipeline,
  title={Omics Cancer Classification Pipeline},
  author={Your Name},
  year={2025},
  url={https://github.com/yourusername/omics-cancer-pipeline}
}
```

---

## ğŸš€ Future Improvements

- [ ] Multi-omics integration (early/intermediate fusion)
- [ ] Transfer learning from pretrained models
- [ ] Attention-based feature selection
- [ ] Graph transformer architectures
- [ ] Survival analysis integration
- [ ] Interactive visualization dashboard
- [ ] Automated hyperparameter tuning (AutoML)
- [ ] Federated learning support

---

**Happy Analyzing! ğŸ§¬ğŸ”¬**
